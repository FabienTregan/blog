= Soon
Julien Kirch
v1.0, 2019-01-01
:article_lang: en

== link:https://runyourown.social[Run your own social: How to run a small social network site for your friends]

[quote]
____
The main reason to run a small social network site is that you can create an online environment tailored to the needs of your community in a way that a big corporation like Facebook or Twitter never could. Yes, you can always start a Facebook Group for your community and moderate that how you like, but only within certain bounds set by Facebook. If you (or your community) run the whole site, then you are ultimately the boss of what goes on. It is harder work than letting Facebook or Twitter or Slack or Basecamp or whoever else take care of everything, but I believe it's worth it.

Let's go back to Friend Camp. While there are a hundred thousand people we can talk to from Friend Camp, there are only about 50 people with an active Friend Camp login. We call ourselves "campers" because we are corny like that. And campers have a special communication channel that lets us post messages that only other campers can see.

If I make software that makes the lives of 50 people much nicer, and it makes 0 people more miserable, then on the balance I think I'm doing better than a lot of programmers in the world.
Because we're mostly all friends with each other, this extra communications mode is kind of like a group chat on steroids. For our community it ends up being a sort of hybrid between Twitter and a group chat. As a result of having a community layer alongside a more public layer, we have a movie night, a book club, and a postcard club. Campers visit each other when we travel, even if we've never met in person before. We correspond with each other about what we're making for dinner and trade recipes. They're the kind of mundane interactions that you probably don't want to have with perfect strangers but you cherish in a group of people you care about.

We are also able to have moderation rules that are hyper-specific to our own values as a community. It lets us maintain an environment that's far more pleasant than you find on most social media sites.
____

== link:https://www.ralfj.de/blog/2019/07/14/uninit.html["`What The Hardware Does`" is not What Your Program Does: Uninitialized Memory]

[quote]
____
Maybe the most important lesson to take away from this post is that "`what the hardware does`" is most of the time _irrelevant_ when discussing what a Rust/C/C++ program does, unless you _already established that there is no undefined behavior_. Sure, hardware (well, link:https://devblogs.microsoft.com/oldnewthing/20040119-00/?p=41003[most hardware]) does not have a notion of "`uninitialized memory`". But _the Rust program you wrote does not run on your hardware_. It runs on the Rust abstract machine, and that machine (which only exists in our minds) _does_ have a notion of "`uninitialized memory`". The real, physical hardware that we end up running the compiled program on is a very efficient but _imprecise_ implementation of this abstract machine, and all the rules that Rust has for undefined behavior work together to make sure that this imprecision is not visible for _well-behaved_ (UB-free) programs. But for programs that do have UB, this "`illusion`" breaks down, and link:https://raphlinus.github.io/programming/rust/2018/08/17/undefined-behavior.html[anything is possible].

_Only_ UB-free programs can be made sense of by looking at their assembly, but _whether_ a program has UB is impossible to tell on that level. For that, you need to think in terms of the abstract machine.footnote:[This does imply that tools like valgrind, that work on the final assembly, can never reliably detect _all_ UB.]

This does not just apply to uninitialized memory: for example, in x86 assembly, there is no difference between "`relaxed`" and "`release`"/`"acquire`"-style atomic memory accesses. But when writing Rust programs, even when writing Rust programs that you only intend to compile to x86, "`what the hardware does`" just does not matter if your program has UB. The Rust abstract machine does make a distinction between "`relaxed`" and "`release`"/`"acquire`", and your program will go wrong if you ignore that fact. After all, x86 does not have "`uninitialized bytes`" either, and still our example program above went wrong.

Of course, to explain _why_ the abstract machine is defined the way it is, we have to look at optimizations and hardware-level concerns. But without an abstract machine, it is very hard to ensure that all the optimizations a compiler performs are consistent—in fact, both link:https://bugs.llvm.org/show_bug.cgi?id=35229[LLVM] and link:https://gcc.gnu.org/bugzilla/show_bug.cgi?id=65752[GCC] suffer from miscompilations caused by combining optimizations that all seem fine in isolation, but together cause incorrect code generation. The abstract machine is needed as an ultimate arbiter that shows if all of the optimizations are correct, or if some of them are in conflict with each other. I also think that when writing unsafe code, it is much easier to keep in your head a fixed abstract machine as opposed to a set of optimizations that might change any time, and might or might not be applied in any order.

Unfortunately, in my opinion not enough of the discussion around undefined behavior in Rust/C/C++ is focused on what concretely the "`abstract machine`" of these languages looks like. Instead, people often talk about hardware behavior and how that can be altered by a set of allowed optimizations—but the optimizations performed by compilers change as new tricks are discovered, and it’s the abstract machines that define if these tricks are allowed. C/C++ have extensive standards that describe many cases of undefined behavior in great detail, but nowhere does it say that memory of the C/C++ abstract machine stores `Option<u8>` instead of the `u8` one might naively expect. In Rust, I am very happy that we have link:https://github.com/rust-lang/miri/[Miri], which is meant to be (very close to) a direct implementation of the Rust abstract machine, and I am strongly advocating for us to write the Rust specification (once that gets going) in a style that makes this machine very explicit. I hope C/C++ will come around to do the same, and there is some link:https://www.cl.cam.ac.uk/~pes20/cerberus/[great work in that direction], but only time will tell to what extend that can affect the standard itself.
____

== link:https://danluu.com/deconstruct-files/[Files are fraught with peril]

[quote]
____
In conclusion, computers don't work (but you probably already know this if you're here at Gary-conf). This talk happened to be about files, but there are many areas we could've looked into where we would've seen similar things.

One thing I'd like to note before we finish is that, IMO, the underlying problem isn't technical. If you look at what huge tech companies do (companies like FB, Amazon, MS, Google, etc.), they often handle writes to disk pretty safely. They'll make sure that they have disks where power loss protection actually work, they'll have patches into the OS and/or other instrumentation to make sure that errors get reported correctly, there will be large distributed storage groups to make sure data is replicated safely, etc. We know how to make this stuff pretty reliable. It's hard, and it takes a lot of time and effort, i.e., a lot of money, but it can be done.

If you ask someone who works on that kind of thing why they spend mind boggling sums of money to enure (or really, increase the probability of) correctness, you'll often get an answer like "we have a zillion machines and if you do the math on the rate of data corruption, if we didn't do all of this, we'd have data corruption every minute of every day. It would be totally untenable". A huge tech company might have, what, order of ten million machines? The funny thing is, if you do the math for how many consumer machines there are out there and much consumer software runs on unreliable disks, the math is similar. There are many more consumer machines; they're typically operated at much lighter load, but there are enough of them that, if you own a widely used piece of desktop/laptop/workstation software, the math on data corruption is pretty similar. Without "extreme" protections, we should expect to see data corruption all the time.

But if we look at how consumer software works, it's usually quite unsafe with respect to handling data. IMO, the key difference here is that when a huge tech company loses data, whether that's data on who's likely to click on which ads or user emails, the company pays the cost, directly or indirectly and the cost is large enough that it's obviously correct to spend a lot of effort to avoid data loss. But when consumers have data corruption on their own machines, they're mostly not sophisticated enough to know who's at fault, so the company can avoid taking the brunt of the blame. If we have a global optimization function, the math is the same -- of course we should put more effort into protecting data on consumer machines. But if we're a company that's locally optimizing for our own benefit, the math works out differently and maybe it's not worth it to spend a lot of effort on avoiding data corruption.
____

== link:http://yosefk.com/blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo.html[Don't ask if a monorepo is good for you – ask if you're good enough for a monorepo]

[quote]
____
But I'm going to make an argument which I think mostly works regardless of the state of tooling on any given day:

* Monorepo is great if you're really good, but absolutely terrible if you're not that good.
* Multiple repos, on the other hand, are passable for everyone – they're never great, but they're never truly terrible, either.
____

== link:https://chadaustin.me/2015/07/sum-types/[Sum Types Are Coming: What You Should Know
]

[quote]
____
Just as all mainstream languages now have lambda functions, I predict sum types are the next construct to spread outward from the typed functional programming community to the mainstream. Sum types are very useful, and after living with them in Haskell for a while, I miss them deeply when using languages without them.

Fortunately, sum types seem to be catching on: both link:https://doc.rust-lang.org/book/enums.html[Rust] and link:https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/Enumerations.html#//apple_ref/doc/uid/TP40014097-CH12-ID146[Swift] have them, and it sounds like TypeScript's developers are at least link:https://developer.apple.com/library/prerelease/ios/documentation/Swift/Conceptual/Swift_Programming_Language/Enumerations.html#//apple_ref/doc/uid/TP40014097-CH12-ID146[open to the idea].

I am writing this article because, while sum types are conceptually simple, most programmers I know don't have hands-on experience with them don't have a good sense of their usefulness.

In this article, I'll explain what sum types are, how they're typically represented, and why they're useful. I will also dispel some common misconceptions that cause people to argue sum types aren't necessary.
____