= Écrire un scrapper de site web en Ruby
Julien Kirch
v0.1, 2020-11-19
:article_lang: fr

Pour mon usage personnel, par exemple télécharger des sites ou des BDs pour les lire hors ligne sur une liseuse, ou pour rendre services à des amis en extrayant des données structurées, j'ai souvent besoin d'écrire des scrapper pour des sites web.

Il serait généralement possible d'utiliser un outil de scrapping générique et de le paramétrer, ou de télécharger l'intégralité du site avec un outil comme link:https://www.httrack.com[HTTrack] puis de traiter les fichiers en local, mais un scrapper maison est souvent plus efficace car on peut facilement ne télécharger que les contenu qui nous intéressent, et extraire à la volée les données dont on a besoin.

Une fois qu'on a une base de code qui fonctionne et qu'on maîtrise bien, le modifier pour un nouveau usage se fait facilement.
C'est un des cas où copier / coller un existant et le rééditer pour chaque nouveau besoin est plus efficace que d'essayer d'avoir une solution générique qu'on aurait "`seulement`" à configurer.

Cet article se propose de vous guider pas à pas jusqu'à avoir une première version complète, dans l'idée de vous rendre autonome.

Par ailleurs, la conception d'un scrapper permet de creuser quelques sujets intéressantes, naturellement un peu de HTML et de HTTP mais aussi de stockage de données.

Avertissement{nbsp}: de nombreux sites indiquent dans leur conditions d'utilisation qu'il n'est pas autorisé de les scrapper.
Je ne sais pas si ces messages ont une valeur légale, mais cet article n'est pas un encouragement à scrapper ces sites.

== Le scrapper à développer

Je vais ici développer un scrapper qui fait de l'archivage de site web.
Son objectif est d'obtenir une copie locale d'un site qui soit navigable en ouvrant les fichiers dans un navigateur.

Pour les fichiers et le parcours du site, le scrapper parcourra le HTML généré côté serveur sans exécuter le JavaScript, et ne sera donc pas compatible avec des sites générés côté client.

Il est possible d'écrire en Ruby des scrapper qui savent exécuter du JavaScript, par exemple avec des outils comme linkhttps://github.com/YusukeIwaki/puppeteer-ruby[Puppeteer Ruby] qui peut piloter une instance de Chrome, mais cela demande plus de travail et j'en ai rarement eu besoin quand j'ai fais du scrapping, je ne traiterai donc pas ce sujet ici.

Comme l'objectif est d'obtenir un code facile à modifier plutôt que facile à étendre, le code sera placé dans un fichier unique est dans un style procédural, c'est-à-dire en évitant de structurer le code à l'aide de classe mais en utilisant uniquement des méthodes.

== Télécharger une seule page

Pour commencer je vais télécharger uniquement le contenu HTML d'une page dont l'adresse est spécifiée dans le code.

Pour le téléchargement j'utilise la classe `Net::HTTP` fournie par la bibliothèque standard Ruby.
Certes il existe de nombreuses librairies externes fournissant des fonctionnalités plus évoluées et/ou des API plus simple, mais pour les besoins de scrapping `Net::HTTP` suffit généralement, et il intéressant de la connaître un peu car elle est utilisée par défaut par de nombreuses librairies.
Pour un scrapper minimal, la seule méthode à connaître est `Net::HTTP#get` qui fait une requête `GET` à partir d'une URL et qui renvoie le contenu.

La classe `URI::HTTP` de la bibliothèque standard Ruby implémente les différents standards mais elle est assez stricte.
Cela signifie qu'elle n'apprécie pas toujours qu'on prenne des liberté avec les standard.
Les navigateurs sont eux plutôt tolérants, et par conséquent les sites webs ne sont pas incités à suivre les standard au pied de la lettre.
Cela signifie que pour du scrapper, mieux un parseur d'URL tolérant, et pour cela je vais utiliser la bibliothèque link:https://github.com/sporkmonger/addressable[Addressable].

Voici donc le code{nbsp}:

.scrapper.rb
[source, ruby]
----
include::scrapper_1.rb[]
----

L'exécution du script devrait créer en local un fichier `index.html` qui contient le source de la page.
La page téléchargée à link:http://example.com[example.com] n'utilisant ni image ni CSS externe, la version locale est auto-portante (en tous cas au moment où j'écris cet article).

== Configurabilité{nbsp}? Non, merci{nbsp}!

Pour un outil classique, la deuxième étape serait de rendre le script configurable, en permettant par exemple de lui passer l'URL d'entrée du site ou le répertoire cible en paramètre.

Mais rappelez vous que mon objectif n'est pas d'avoir un script configurable mais un script facilement éditable.
Avoir une URL en dur dans une constante en début de fichier fait donc très bien l'affaire.

== Télécharger une page et ses dépendances

À la place, la deuxième étape va consister à télécharger une page et ses dépendances externes{nbsp}: images, feuilles de style et fichiers JavaScript.

Je vais prendre pour cible le site du magazine link:https://queue.acm.org[ACM Queue] qui est un magazine publiant des articles d'ingénierie logicielle.

== URLS et noms de fichiers

Mais d'abord il me faut parler des URLs et des noms de fichiers.

link:http://www.faqs.org/rfcs/rfc1738.html[Il y a très longtemps], les URLs ne pouvaient quasiment utiliser que des caractères alphanumériques.
Les dinosaures régnaient sur le monde et la vie était simple.

Désormais on peut avoir des URLs avec des loutres comme link:https://emojipedia.org/emoji/🦦/[https://emojipedia.org/emoji/🦦/].

Dans un scrapper qui archive un site chaque contenu est sauvegardé.
Si cette sauvegarde est faite dans une base de données classique, comme par exemple une base SQL, vous pouvez stocker les URLs dans un champ de texte, même si elles contiennent des loutres.

Si la base de donnée utilisée par le stockage est un système de fichier, les choses peuvent être moins simple.

Les systèmes d'exploitation peuvent être peu contraignants sur les formats de noms de fichiers (par exemple avec le système de fichiers ext4 souvent utilisé sous Linux tous les caractères sont autorisés à part `NULL` et `/`).

Mais en pratique vous n'avez peut-être pas envie d'avoir sur votre disque dur ü͉̞͖͇̥̫̊͞n̢̟͖̺̗̥̱̬̅̾̿ͅ ̬̑̀ f͆ͧ҉̺̪͚̩̭̭̙i̵̤̟͚̳̠̟̣̬̽̊̑c͎̳̘̟̼͊ͤ͠h̫̫̎̒ͯͪ͘ͅi̦͉̞̩̠̫̲̅ͥ̀͠e̵͚̘̒r̹̝͔̪͉̙̒͑ͦ͞ ͉̲͓̘͌͢ͅq̶͈̺͈̫̜͎͉͉̌͊̍̚u̫̤͖̼̮̝͐ͤ͢ḭ̱͕͔̮̗̲̂͂̇̽̕ ̈́͌҉̭͎̪ͅṣ̨͕͇͕̯̗̗̭͙̭̳̼̖̄̐͋͂͡'̴̩̥̭̤̫̖̈̐a̳̙͍̯ͩ̆̽͛́p̝̮̦̹͇̥ͪ͗̂͟p͈͙͓̻ͤͭ͟e̪͆ͬ̆ͭ̕ͅl͈̩̜̓ͫ̕l̴̞̟̼͕͙̮̤̺̊̓ͣe̷͇̰͙͒̔͛̚ ̭̻̰͇̖̆͡c̨̬̖̥̖̽ͪ̈́o̢͈̲̭͈̟̫̭̒̂̾ͤm̭͉̩͔͎̼̳͖ͧ͡m̴͎̙̳̟̖͆̔ę͇̲̻̠͎̊͂ͬ̊ͅ ̨͕͔͕̹̼̓̒̃�̛͎̜͇̹̻̰͚̹ͧͩ�̭̗͓͖̤̖̬ͫ̆̑͞a̡̪͍̪̍

Cela signifie qu'il faut trouver une opération permettant de transformer les URLs en nom de fichier acceptables.

On pourrait vouloir supprimer les caractères qu'on veut éviter en ne conservant que les caractères compatibles.
C'est ce qui est est fait dans certains CMS pour transformer des noms de fichiers en URLs.

L'inconvénient de cette méthode est qu'elle supprime des informations (les caractères non acceptables), et qu'en faisait ça deux URLs différentes peuvent être transformer en un nom de fichier identique.

`https://emojipedia.org/emoji/🦦/` et `https://emojipedia.org/emoji/🦔/` auraient ainsi le même nom de fichier.

L'opération de transformation doit donc avoir les caractéristiques suivantes{nbsp}:

- une même URL doit toujours about au même nom de fichier (pour éviter d'avoir des doublons)
- un nom de fichier doit correspondre à une seule URL (pour éviter le cas décrit plus haut)

En mathématique, ce type de transformation est appelé link:https://fr.wikipedia.org/wiki/Bijection[bijection].

Une manière de mettre en place cette transformation est d'utiliser un dictionnaire, c'est-à-dire une source de données qui permette de stocker la correspondance entre URL en nom de fichier.
C'est un des usages des bases de données SQL{nbsp}:
on peut ainsi créer une table `URL` avec une colonne qui contient l'URL originale avec une clé d'unicité, et une colonne contenant un incrément automatique (aussi appelé séquence).
Lorsqu'on veut récupérer le nom de fichier correspondant à une URL, on commence par vérifier si cette URL est déjà dans la table, et sinon on l'insère.
La valeur de la colonne contenant l'incrément automatique fournit alors des identifiant adaptés à des fichiers, on aurait ainsi des fichiers `1`, `2`…
C'est comme si le nom de fichier était une clé étrangère vis-à-vis de cette table.

Sans utiliser de base de donnée externe, on pourrait gérer ce dictionnaire dans un fichier, par exemple un fichier JSON, ou même un fichier textuel contenant une URL par ligne en utilisant le numéro de ligne comme identifiant pour le fichier.

L'approche dictionnaire à un inconvénient{nbsp}: elle demande de passer par une source de donnée supplémentaire qui n'est pas un des fichiers du site.

Une autre solution est de trouver une transformation qui réponde aux deux caractéristiques décrite ci-dessous et qui se passe de dictionnaire.

Encoder les URL au format link:https://fr.wikipedia.org/wiki/Base64[base64], souvent utilisé sur le web pour encoder des resources binaire dans un contenu textuel est un bon candidat car cela ne repose pas sur un dictionnaire et répond aux deux caractéristiques, mais il a deux inconvénients qui empêchent de s'en servir :

- Il utilise le caractère `/` qui n'est pas autorisé dans des noms de fichiers.
- Il utilise des majuscules et des minuscules, alors que certains systèmes de fichiers (notablement celui de macOS) ne font pas cette distinction.

Un format d'encodage qui n'a pas ces inconvénients est d'utiliser les numéros des caractères composant l'URL (dont le nom officiel est link:https://fr.wikipedia.org/wiki/Point_de_code[point de code]). Comme il s'agit d'une suite de valeurs numériques cela convient bien à des noms de fichiers. On peut même utiliser la représentation hexadécimale de ces nombres pour avoir des noms de fichiers moins longs.

En Ruby, la méthode `String#ord` permet de récupérer le point de code d'un caractère, voici comment faire la transformation{nbsp}:

[source, ruby]
----
url = 'https://emojipedia.org/emoji/🦦/'
hexadecimal_codepoints = url.chars.map do |character| 
  character.ord.to_s(16)
end
file_name = hexadecimal_codepoints.join('-')
----

Ce qui nous donne `68-74-74-70-73-3a-2f-2f-65-6d-6f-6a-69-70-65-64-69-61-2e-6f-72-67-2f-65-6d-6f-6a-69-2f-1f9a6-2f`.

Il est nécessaire de mettre des tirets entre les nombres pour pouvoir distinguer les valeurs successives, car elles n'ont pas toutes le même nombre de chiffres.

Si besoin, voici comment réaliser l'opération inverse{nbsp}:

[source, ruby]
----
file_name = '68-74-74-70-73-3a-2f-2f-65-6d-6f-6a-69-70-65-64-69-61-2e-6f-72-67-2f-65-6d-6f-6a-69-2f-1f9a6-2f'
url = file_name.split('-').map do |hexadecimal_codepoint|
  hexadecimal_codepoint.to_i(16).chr(Encoding::UTF_8)
end.join('')
----

== Télécharger une page et ses dépendances (pour de vrai)

Maintenant qu'on sait comment transformer une URL en nom de fichier, on peut s'intéresser au téléchargement proprement dit.

Pour télécharger les dépendances externes d'une page, le mécanisme est le suivant{nbsp}:

. Identifier la liste des dépendances
. Pour chaque dépendance
.. Si elle n'a pas déjà été téléchargée, la télécharger et la stocker sous le nom calculé en utilisant le code ci-dessus
.. Modifier le HTML de la page pour remplacer l'URL de la dépendance par le chemin du fichier

Pour identifier la liste des dépendances, il faut parcourir le HTML.
Pour cela la bibliothèque Ruby la plus utilisée est link:https://nokogiri.org[Nokogiri], elle sait parser du HTML et du XML et fournit ensuite une interface permettant de requêter et de modifier le contenu.

La première étape est de parser le contenu du HTML que l'on reçoit, et de préparer le fait que la sauvegarde va se faire dans un sous-répertoire pour éviter de mélanger le site avec le scrapper.

Comme l'objectif est de scrapper le magazine ACM Queue, j'ai aussi remplacé l'URL.

.scrapper.rb
[source, ruby]
----
include::scrapper_2.rb[]
----

Mais, quand on essaie d'ouvrir le fichier `download/index.html` résultant{nbsp}:

image::blocked.png[]

Pas de panique{nbsp}: il s'agit d'un mécanisme rudimentaire pour bloquer les robots.

Quand ce genre de choses arrive, il vaut mieux faire deux choses{nbsp}:

- utiliser une bibliothèque HTTP qui a un comportement plus proche de celui des navigateurs
- fournir une en-tête link:https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent[User-Agent] pour accentuer la ressemblance avec un navigateur.

Si vous avez l'habitude d'utiliser une des bibliothèques HTTP qui est faite pour des appels REST, elle risque d'avoir le même problème, car une API REST n'a pas vocation à bloquer les clients qui ne sont pas des navigateurs.

Je vais passer de `Net::HTTP` à la bibliothèque link:https://github.com/taf2/curb[Curb] qui fournit une API Ruby au dessus de la bibliothèque libcurl, que vous connaissez peut-être à travers la commande `curl` utilisée pour faire des téléchargement depuis la ligne de commande.

.scrapper.rb
[source, ruby]
----
include::scrapper_3.rb[]
----

Et ça fonctionne{nbsp}!

image::success_no_dependency.png[]

Le HTML est là, même si pour le moment sans CSS ni image.

Si vous allez jeter un œil au fichier HTML, une petite surprise vous attend{nbsp}:

[source, xml]
---
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<script type="text/javascript">/* <![CDATA[ */_cf_loadingtexthtml="<img alt=' ' src='/cf_scripts/scripts/ajax/resources/cf/images/loading.gif'/>";
---

En effet le contenu n'est pas au format HTML mais au format XHTML.
Il s'agit à peu près d'une version XML de HTML4, créé par le W3C à l'époque où le XML était à la monde.
Les deux avantages de cette approche étaient de pouvoir utiliser des outils XML pour pouvoir publier des sites (ce qui permettait notamment d'avoir la même chaîne de publication pour les livres que pour le contenu web) et de pouvoir utiliser des parsers XML, plus performants que les parsers HTML (cela prend son sens quand on se souvient des téléphones mobiles disponibles dans les années 2000).
La vague du XML sur le web a été stoppée lorsqu'Apple, Mozilla et Opera ont décidé de monter un putsch contre le W3C en créant leur propre instance et leur propre format qui a abouti au HTML5.

Pour le scrapper cela n'a aucune conséquence car Nokogiri traite le HTML et le XHTML de la même manière.

Je commence par déplacer le code de téléchargement dans une méthode séparée, car je vais en avoir besoin pour les ressources externes{nbsp}:

.scrapper.rb
[source, ruby]
----
# @param [Addressable::URI] url
# @return [String]
def download_url_content(url)
  response = Curl.get(url) do |http|
    # Je suis un navigateur web !
    http.headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/81.0'
  end
  response.body_str
end
----

Ensuite je vais commencer par récupérer les images.

Le code suit exactement les étapes décrites plus haut, pour commencer je ne vais pas tenter de factoriser de code.

Pour la recherche dans le HTML, Nokogiri propose une méthode `.css` qui link:https://nokogiri.org/tutorials/searching_a_xml_html_document.html[permet d'utiliser la syntaxe CSS]{nbsp}:

.scrapper.rb
[source, ruby]
----
# Localise les éléments img
doc.css('img').each do |image|
  image_src = image['src']
  # Assure d'avoir une URL absolue en combinant l'adresse de l'image
  # avec celle de la page si l'image a une adresse relative,
  # par exemple http://exemple.com +  lapin.png = http://exemple.com/lapin.png
  # si l'image a déjà une adresse absolue alors utilise celle là
  # Addressable::URI#normalize essaie de corriger les URls incorrectes, par exemple celles qui contiennent des espace
  image_url = parsed_url.join(image_src).normalize
  puts "Télécharge [#{image_url}]"

  # Calcule le nom du fichier
  hexadecimal_codepoints = image_url.to_s.chars.map do |character|
    character.ord.to_s(16)
  end
  image_file_name = hexadecimal_codepoints.join('-')
  image_file_path = File.join(TARGET_DIRECTORY, image_file_name)

  # Si le fichier n'existe pas encore, c'est qu'il faut télécharger l'image
  unless File.exists?(image_file_path)
    IO.write(
      image_file_path,
      download_url_content(image_url)
    )
  end
  # Remplace l'URL d'origine par le nom du fichier
  image['src'] = image_file_name
end
----

En lançant le script, les fichiers images sont bien crées{nbsp}:

image::images_files.png[]

Elles sont bien remplacées dans le HTML{nbsp}:

[source, html]
----
        <a href="index.cfm"><img
                src="68-74-74-70-73-3a-2f-2f-71-75-65-75-65-2e-61-63-6d-2e-6f-72-67-2f-69-6d-67-2f-61-63-6d-71-75-65-75-65-5f-6c-6f-67-6f-2e-67-69-66"></a>
        <br>
        <a href="whyjoinacm.cfm"><img
                src="68-74-74-70-73-3a-2f-2f-71-75-65-75-65-2e-61-63-6d-2e-6f-72-67-2f-69-6d-67-2f-6c-6f-67-6f-5f-61-63-6d-2e-67-69-66"
                border="0" style="clear:both;"></a>
----

Et quand on visualise le fichier, les images s'affichent bien{nbsp}:

image::images_display.png[]

Reste à faire de même pour les feuilles CSS et le JavaScript. Ce qui donne l'occasion de factoriser un peu le code.

Pour faciliter un peu le parcours des fichiers, je vais également ajouter les extensions des URLs aux noms des fichiers, on aura ainsi des fichiers `.png` ou `.js`, en faisant le pari que les sites n'utilisent pas dans leurs extensions d'emoji ou d'autres caractères à éviter dans les noms de fichiers.

Par courtoisie pour éviter de trop solliciter le serveur et potentiellement de se faire bloquer, je vais mettre une seconde d'attente après chaque téléchargement.

.scrapper.rb
[source, ruby]
----
puts "Downloading [#{MAIN_URL}]"
parsed_url = Addressable::URI.parse(MAIN_URL)
doc = Nokogiri::HTML(download_url_content(parsed_url))

# @param [Addressable::URI] html_page_url
# @param [String] resource_url
# @return [String]
def scrape_resource(html_page_url, resource_url)
  # Assure d'avoir une URL absolue en combinant l'adresse de la resource
  # avec celle de la page
  absolute_url = html_page_url.join(resource_url).normalize
  puts "Checking resource [#{absolute_url}]"

  hexadecimal_codepoints = absolute_url.to_s.chars.map do |character|
    character.ord.to_s(16)
  end
  file_name = "#{hexadecimal_codepoints.join('-')}#{File.extname(absolute_url.path)}"
  target_file_path = File.join(TARGET_DIRECTORY, file_name)
  unless File.exists?(target_file_path)
    puts "Télécharge [#{absolute_url}] to [#{target_file_path}]"
    IO.write(target_file_path, download_url_content(absolute_url))
    # Attendre un peu
    sleep(1)
  end
  file_name
end

doc.css('img').each do |image|
  image['src'] = scrape_resource(parsed_url, image['src'])
end

doc.css('link').each do |link|
  # Télécharge seulement les feuilles de styles externes
  if (link['rel'] == 'stylesheet') && link.key?('href')
    link['href'] = scrape_resource(parsed_url, link['href'])
  end
end

doc.css('script').each do |script|
  # Télécharge seulement les scripts externes
  if script.key?('src')
    script['src'] = scrape_resource(parsed_url, script['src'])
  end
end

IO.write(File.join(TARGET_DIRECTORY, 'index.html'), doc.to_html)
----

Et voilà{nbsp}:

image::first_page_with_resources.png[]

== Télécharger le reste du site