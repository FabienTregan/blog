= Comment se mettre à l'échelle en présence d'erreurs -- ne les ignorez pas
tef
v0.1, 2020-06-01
:article_lang: fr
:article_image: avatar.jpg
:faf: fire-and-forget
:bus: bus de messages

Ceci est une traduction d'link:https://programmingisterrible.com/post/188942142748/scaling-in-the-presence-of-errorsdont-ignore[un article] de link:http://twitter.com/tef_ebooks[tef].

''''

Construire un service fiable et robuste signifie souvent construire quelque chose qui peut continuer à fonctionner quand certaines parties sont défaillantes.
Un site web où certaines des fonctionnalités ne fonctionnent pas vaut souvent mieux qu'un site complètement indisponible.
La bonne manière de s'y prendre n'est pas évidente.

La réponse habituelle est d'embaucher plus de DBAs, plus de SREs, et encore plus de personnes pour s'occuper du support.
La gestion d'erreur, autrement dit concevoir des logiciels qui peuvent faire de la reprise sur erreur, semble souvent à l'option en dernier recours, et encore faut-il qu'elle soit envisagée.

La réponse habituelle à la gestion d'erreur est l'optimisme.
Malheureusement, les autres possibilités ne sont pas forcément clait, et faire son choix est souvent difficile.
Si vous avez deux services, que faites-vous quand l'un des deux est indisponible{nbsp}: Rééssayer plus tard{nbsp}? Abandonner complètement{nbsp}? Ou juste l'ignorer et espérer que le problème s'en aille{nbsp}?

Étonnamment, toutes ces approches peuvent être raisonnables.
Même ignorer les problèmes peut fonctionner pour certain systèmes.
Plus ou moins.
Vous ne pouvez pas ignorer les erreurs, mais parfois la reprise après une erreur peut ressembler beaucoup à l'ignorer.

''''

Imaginez un verger rempli de capteurs mesurant la chaleur, la lumière et l'humidité.
Cela n'aurait pas beaucoup de sens d'essayer de renvoyer des températures passées en cas d'erreut.
Ce n'est pas le rôle du capteur de s'assurer que le système fonctionne, et le capteur n'y peut pas grand chose non plus.
Par conséquent il est raisonnable pour un capteur d'envoyer des messages sans se compliquer la vie, autrement dit en mode _{faf}_ ("`tire et oublie`" où le système qui envoie les message ne se préoccupe pas de savoir ce qui leur arrive).

L'idée derrière le _{faf}_ c'est que vous n'avez pas besoin de sauvegarder les anciens messages quand le prochain message est disponible, ou qu'un message manquant ne va pas causer de problème.
Dans cette situation chaque message est traité comme s'il était le premier message envoyé, en oubliant toute tentative qui aurait eu lieu plus tôt.

Bien fait, le _{faf}_ est comme la réunion journalière (_daily meeting_){nbsp}: si une personne manque la réunion, elle peut participer à celle qui se tiendra le jour d'après.
Mal fait, le _{faf}_ est comme de hurler dans le bureau au lieu d'envoyer un mail en espérant que quelqu'un prendra des notes.

Ce n'est pas qu'il n'y a pas de gestion d'erreur dans un client _{faf}_, mais que de simplement continuer est la meilleur méthode de reprise sur erreur.
Malheuresement, _{faf}_ est souvent interprété à tord comme signifiant "`passons-nous de toute gestion d'erreur et espérons que tout se passe au mieux`".

Vous ne pouvez pas vous permettre d'ignorer les erreurs.

Quand vous ignorez les erreur, tout ce que vous faites c'est retarder le moment où vous les découvrez{nbsp}: c'est seulement lorsuqu'un autre problème est causé par le premier que quelqu'un réalisé que quelque chose s'est mal passé.
Quand vous ignorez les erreur, vous gaspillez du temps qui pourrait être utilisé pour restaurer la situation.

C'est pour cela que, malgrès quelques contre-exxemples, la meilleur chose à faire quand vous rencontrez une erreur est d'abandonner.
Arrêtez avant de faire empirer les chose, et laisser quelque chose d'autre s'en occuper.

''''

Abandonner est une approche surprenament raisonnable de la gestion d'erreur, en supposant que quelque chose d'autre essaiera de reprendre, redémarrer ou de faire continuer le programme.
C'est pour cela que presque tous les services réseaux s'executent dans une boucle qui les fait redémarrer immediatement en cas de plantage, en esperant que l'erreur est temporaire.
Elle l'est souvent.

Cela ne sert pas à grand chose d'essayer de se reconnecter plusieurs fois à une base de données quand l'utilisateur·rice est déjà en train d'essayer de cliquer en boucle sue le bontn "`rafraîchir`" de son navigateur.
Un pipeline Unix pourrait gérer tous les cas d'erreurs posibles, mais le plus souvent relancer le programme suffit à tout faire fonctionner.

Bien qu'abandonner soit une bonne manière de gérer les erreur, redémarrer de zero n'est pas toujours la meilleure manière de faire de la reprise.

Certains pipelines fonctionnent sur des gros volumes de données, ou font des tas de traitements numériques compliqués, et personne n'est jamais content de devoir répérer des jours ou des semaines de travail.
En théorie, vous pourriez ajouter de la gestion d'erreur, réduire le risque que le programme se plante, et éviter un redémarrage coûteux, mais en pratique il est souvent plus facile de réorganiser votre code pour qu'il continue là où il s'est arrêté.

En d'autres termes{nbsp}: abandonnez, mais sauvegardez votre progression pour que le redémarrage prenne moins de temps.

Pour un pileline, cela signifie généralement un horrible tas de fichiers temporaires pour sauvegarder le résultat de l'execution de chaque sous-commande, et le résultat du découpage des données d'entrée en lots plus petits.
Vous pouvez même mettre en place des rééssais automatiques, mais pour de nombreux pipelines, une reprise manuelle est toujours relativement peu coûteuse.

Pour d'autres tâches à durée de vie longue, cela signifie habituelle quelque chose comme des points de contrôles ou des sagas.
En d'autres termes, transformer un traitement long en un traitement court qui s'execute constamment, en écrivant les progrès effectués dans un fichier ou une base de données quelque part.

Avec le temps, tous les traitements longs sont découpés en parties plus petites, quand le coût de les redémmarrer de zero devient prohibitif.
Un traitement long a beaucoup plus de chances de tomber sur une erreur impossible&#8201;—{nbsp}des  disques pleins, plus de mémoire disponibles, des rayons cosmiques{nbsp}—&#8201;et être forcé d'abandonner.

Parfois la seule manière de gérer une erreur est d'abandonner.

Par conséquent, la meilleure manière de gérer les erreurs est d'organiser vos programmes de façon à rendre les reprises faciles.
La reprise c'est tout la différence entre "`_{faf}_`" et "`ignorer-toutes-les-erreurs`", même si les deux partagent le même optimisme.

Vous pouvez faire des choses qui ressemblent au fait d'ignorer des erreurs, ou même laisser quelque chose d'autre s'en occuper, tant qu'il y a un plan pour pouvoir faire une reprise.
Même si c'est de recommencer de zéro, même si c'est de réveiller quelqu'un pendant la nuit, tant qu'il y a un plan cela signifie que vous n'ignorez pas le problème.
En supposant bien entendu que le plan fonctionne.

Vous ne pouvez pas ignorer les erreurs.
Elles sont inévitablement le problème _de quelqu'un_.
Si une personne vous dit qu'elle peut ignorer les erreurs, elle vous dit que quelqu'un d'autre est d'astreinte pour leur logiciel.

Ça, ou alors qu'elle utilise un {bus}.

''''

Un {bus}, si vous n'en êtes pas sûr·e, est un service réseau qui propose un ensemble de queues avec lesquelles d'autres ordinateurs sur le réseau peuvent interragir.
Généralement des clients ajoutent des message, et d'autres l'interrogent pour récupérer le prochain message non lu, mais on peut les utiliser d'un tas d'autres manières.

Comme un _pipe_ unix, les {bus} sont utilisé pour démarrer des projets.
De la même manière que les fichiers temporaires, les bus permettent à différentes parties du pipeline de consommer et de produire des contenus à des vitesses différentes, mais ne permettent pas facilement de faire des rejeux ou de redémarrer en cas d'erreur.

Comme un _pipe_ unix, les {bus} sont utilisés d'une manière très optimiste{nbso}:
envoyer des messages dans la queue et passer à la tâche suivante.

À peu près comme un pipeline unix, mais avec des différences notables.
Un pipeline unix se bloque quand il est plein, mettant en pause le producteur jusqu'à ce que le consommateur comble son retard.
Un pipeline unix s'arrête si une des sous-commandes s'arrête, et retourne une erreur si la dernière sous-commande échoue.

Un {bus} ne bloque pas le producteur jusqu'à ce que le consommateur comble son retard.
En théorie, cela signifie que les erreurs temporaires ou les erreurs réseau entre les composants ne plantent pas tout le système.
En pratique, plus vous avez de queues dans un pipeline, plus il faut de temps pour déterminer s'il y a un problème.

Parfois ça fonctionne.
Quand il n'y a pas de croissance, les {bus} agissent comme des tampons entre les parties du système, gérant les variations de charges.
Ils fonctionnent bien pour ralentir les clients qui fonctionnent en rafale, et peuvent fournir un point central pour l'audit ou le contrôle d'accès.

Quand il y a de la coissance, les queues explosent régulièrement jusqu'à ce qu'une forme de limite de débit apparaîsse.
Quand plus de charge arrive, les queues sont partitionnées, et ensuite repartitionnées.
Mettre à l'échelle un {bus} mène inévitablement à limiter la taille de la queue ou à même à la rendre éphémère.

Le problème avec l'optimisme est que quand les choses se passent mal, non seulement vous n'avez aucune idée de la manière de corriger, mais vous ne savez même pas ce qui s'est mas passé.
Dans une certaine limite, un {bus} cache les erreurs&#8201;—{nbsp}les programmes peuvent venir et s'en aller comme ils le veulent, et il n'y aucun moyen de savoir si l'autre partie du système est toujours en en train de lire vos messages{nbsp}—&#8201;, mais il peut seulement cacher les erreurs pendant un certain temps.

En d'autres termes, _fire-and-regret_ ("`tire et regrette`").

Bien qu'une queue sans limite de taille soit une abstraction tentate, elle réalise rarement le fantasme de vous libérer du besoin de gérer les erreurs.
À l'inverse d'un pipeline unix, un {bus} remplira toujours votre disque avant d'abandonner, et modifier les choses pour rendre la reprise facile que d'ajouter plus de fichiers temporaires.

Les {bus} peuvent se remettre d'une seule erreur&#8201;—{nbsp}une panne réseau temporaire{nbsp}—&#8201;alors il faut ajouter d'autre mécanisme pour compenser.
Durées d'expirations, rééssais, et parfois une deuxième queue "`prioritaire`", parce que le blocage en tête de file est quelque chose de réellement horrible à gérer.
En plus, si un traitement se plante, des messages peuvent être perdus.

Les queue aident rarement à la reprise.
Elles la gènent fréquément.

Imaginez un pipeline de build, ou un système de tâches en arrière-plan ou les requêtes sont balancées dans une queue sans se poser de questions.
Quand quelque chose casse, ou ne fonctionne pas comme cela devrait, vous n'avez aucune idée de l'endroit où commencer la reprise.

Avec une queue en arrière-plan, vous ne savez pas quelles sont les tâches qui sont en train d'être executées en ce moment.
Vous ne pouvez pas dire si quelque chose est en train d'être rééssayé, ou a échoué, mais peut-être que vous avez des fichiers de log que vous pouvez fouiller.
Avec des logs, vous pouvez voir ce que le système faisait quelques minutes plus tôt, mais vous n'avez toujours aucune idée de ce qu'il est en train de faire en ce moment.

Même si vous connaissez la taille d'une queue, vous allez devoir regarder le tableau de bord quelques minutes plus tard&#8201;—{nbsp}pour voir si la ligne a bougé{nbsp}—&#8201;avant d'être certain·e que les choses fonctionnent probablement. Avec un peu de chance.

Créer un pipeline de build avec des queues est relativement facilement, mais en construire un où les utilisateur·rice·s peuvent annuler des tâches ou surveiller ce qui se passe demande beaucoup plus de travail.
Dès que vous voulez annuler ou inspecter une tâche, vous devez garder des choses ailleurs que dans une queue.

Savoir qu'une programme est en train de faire signifie suivre les éléments intermédiaires, et même pour quelque chose d'aussi simple que d'executer une tâche en arrière-plan, cela peut nécessiter de nombreux états&#8201;—{nbsp}créé, dans la queue, en cours de traitement, terminé, en échec, et pas seulement dans la queue{nbsp}—&#8201;et un {bus} gère seulement ce dernier cas.

Et ensuite les chose se gâtent.
Dès qu'une queue en remplit une autre, une unité de travail peut être dans plusieurs queues différentes.
Si un élément n'est pas dans la queue, vous savez qu'il a été supprimé ou traité, si un élément est dans la queue, vous ne savez pas s'il est en train d'être traité, mais souvez savez qu'il le sera.
Une queue ne se contente pas de cacher les erreur, elle cache aussi les états.

Pour pouvoir faire une reprise il faut savoir dans quel état était le programme avant que les choses ne se passent mal, et quand vous utilisez le _{fof}_ dans une queue, vous abandonnez l'idée de savoir ce qui se passe ensuite.
Gérer des erreur, reprendre après des erreurs, signifie construire des logiciels qui peuvent savoir quel est leur état.
Cela signifie aussi structurer les choses pour que la reprise soit possible.

That, or you give up on on automated recovery of almost any kind. In
some ways, I'm not arguing against fire-and-forget, or against
optimism—but against optimism that prevents recovery. Not against
queues, but how queues inevitably get used.

Unfortunately, recovery is relatively easy to imagine but not
necessarily straight forward to implement.

This is why some people opt to use a replicated log, instead of a
message broker.

''''

If you've never used a replicated log, imagine an append only database
table without a primary key, or a text file with backups, and you're
close. Or imagine a message broker, but instead of enqueue and dequeue,
you can append to the log or read from the log.

Like a queue, a replicated log can be used in a fire-and-forget fashion
with not so great consequences. Just like before, chaos will ensue as
concepts like rate-limiting, head-of-line blocking, and the
end-to-end-principle are slowly contended with—If you use a replicated
log like a queue, it will fail like a queue.

Unlike a queue, a replicated log can aid recovery.

Every consumer sees the same log entries, in the same order, so it's
possible to recover by replaying the log, or by catching up on old
entries. In some ways it's more like using temporary files instead of a
pipeline to join things together, and the strategies for recovery
overlap with temporary files, too—like partitioning the log so that
restarts aren't as expensive.

Like temporary files, a replicated log can aid in recovery, but only to
a certain point. A consumer will see the same messages, in the same
order, but if a entry gets dropped before reaching the log, or if
entries arrive in the wrong order, some, or potentially all hell can
break loose.

You can't just fire-and-forget into a log, not over a network. Although
a replicated log is ordered, it will preserve the ordering it gets,
whatever that happens to be.

This isn't always a problem. Some logs are used to capture analytic
data, or fed into aggregators, so the impact of a few missing or out of
order entries is relatively low—a few missing entries might as well be
called high-volume random sampling and declared a non-issue.

For other logs, missing entries could cause untold misery. Recovering
from missing entries might involve rebuilding the entire log from
scratch. If you're using a replicated log for replication, you probably
care quite a lot about the order of log entries.

Like before, you can't ignore errors—you only make things expensive to
recover from.

Handling errors like out of order or missing log entries means being
able to work out when they have occurred.

This is more difficult than you might imagine.

''''

Take two services, a primary and a secondary, both with databases, and
imagine using a replicated log to copy changes from one to another.

It doesn't seem to difficult at first. Every time the primary service
makes a change to the database, it writes to to log. The secondary reads
from the log, and updates its database. If the primary service is a
single process, it's pretty easy to ensure that every message is sent in
the right order. When there's more than one writer, things can get
rather involved.

Now, you could switch things around—write to the log first, then apply
the changes to the database, or use the database's log directly—and
avoid the problem altogether, but these aren't always an option.
Sometimes you're forced to handle the problem of ordering the entries
yourself.

In other words, you'll need to order the messages before writing them to
the log.

You could let something else provide the order, but you'd be mistaken if
you think a timestamp would help. Clocks move forwards and backwards and
this can cause all sorts of headaches.

One of the most frustrating problems with timestamps is ‘doomstones':
when a service deletes a key but has a wonky clock far out in the
future, and issues an event with a similar timestamp. All operations get
silently dropped until the deletion event is cleared. The other problem
with timestamps is that if you have two entries, one after the other,
you can't tell if there are any entries that came between them.

Things like "`Hybrid Logical Clocks`", or even atomic clocks can help to
narrow down clock drift, but only so much. You can only narrow down the
window of uncertainty, there's still _some_ clock skew. Again, clocks
will go forwards and backwards—timestamps are terrible for ordering
things precisely.

In practice you need explicit version numbers, 1,2,3… etc, or a unique
identifier for each version of each entry, and a link back to the record
being updated, to order messages.

With a version number, messages can be reordered, missing messages can
be detected, and both can be recovered from, although managing and
assigning those version numbers can be quite difficult in practice.
Timestamps are still useful, if only for putting things in a human
perspective, but without a version number, it's impossible to know what
precise order things happened in—and that no steps are missing, either.

You don't get to ignore errors, but sometimes the error handling code
isn't that obvious.

Using version numbers or even timestamps both fall under building a plan
for recovery. Building something that can continue to operate in the
presence of failure. Unfortunately, building something that works when
other parts fail is one of the more challenging parts of software
engineering.

It doesn't help that doing the same thing in the same order is so
difficult that people use terms like causality and determinism to make
the point sink in.

You don't get to ignore errors, but no one said it was going to be easy.

''''

Although using things like replicated logs, message brokers, or even
using unix pipes can allow you to build prototypes, clear demonstrations
of how your software works—they do not free you from the burden of
handling errors.

You can't avoid error handling code, not at scale.

The secret to error handling at scale isn't giving up, ignoring the
problem, or even it trying again—it is structuring a program for
recovery, making errors stand out, allowing other parts of the program
to make decisions.

Techniques like fail-fast, crash-only-software, process supervision, but
also things like clever use of version numbers, and occasionally the odd
bit of statelessness or idempotence. What these all have in common is
that they're all methods of recovery.

Recovery is the secret to handling errors. Especially at scale.

Giving up early so other things have a chance, continuing on so other
things can catch up, restarting from a clean state to try again, saving
progress so that things do not have to be repeated.

That, or put it off for a while. Buy a lot of disks, hire a few SREs,
and add another graph to the dashboard.

The problem with scale is that you can't approach it with optimism. As
the system grows, it needs redundancy, or to be able to function in the
presence of partial errors or intermittent faults. Humans can only fill
in so many gaps.

Staff turnover is the worst form of technical debt.

Writing robust software means building systems that can exist in a state
of partial failure (like incomplete output), and writing resilient
software means building systems that are always in a state of recovery
(like restarting)—neither come from engineering the happy path of your
software.

When you ignore errors, you transform them into mysteries to solve.
Something or someone else will have to handle them, and then have to
recover from them—usually by hand, and almost always at great expense.

The problem with avoiding error handling in code is that you're only
avoiding automating it.

In other words, the trick to scaling in the presence of errors is
building software around the notion of recovery. Automated recovery.

That, or burnout. Lots of burnout. You don't get to ignore errors.